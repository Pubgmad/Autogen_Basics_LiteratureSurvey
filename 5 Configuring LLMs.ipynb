{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring LLMs in AutoGen v0.4+\n",
    "Starting with OpenAIâ€™s GPT-4.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the Chat GPT / OpenAI code here\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "model_client = OpenAIChatCompletionClient(model='gpt-4o', api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GEMINI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "response = await model_client.create([UserMessage(content=\"Who are you?\", source=\"user\")])\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Assuming your Ollama server is running locally on port 11434.\u001b[39;00m\n\u001b[32m      5\u001b[39m ollama_model_client = OllamaChatCompletionClient(model=\u001b[33m\"\u001b[39m\u001b[33mllama3.2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m ollama_model_client.create([UserMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhat is the capital of France?\u001b[39m\u001b[33m\"\u001b[39m, source=\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m)])\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ollama_model_client.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\autogen\\autogen3\\Lib\\site-packages\\autogen_ext\\models\\ollama\\_ollama_client.py:646\u001b[39m, in \u001b[36mBaseOllamaChatCompletionClient.create\u001b[39m\u001b[34m(self, messages, tools, tool_choice, json_output, extra_create_args, cancellation_token)\u001b[39m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    645\u001b[39m     cancellation_token.link_future(future)\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m result: ChatResponse = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    648\u001b[39m usage = RequestUsage(\n\u001b[32m    649\u001b[39m     \u001b[38;5;66;03m# TODO backup token counting\u001b[39;00m\n\u001b[32m    650\u001b[39m     prompt_tokens=result.prompt_eval_count \u001b[38;5;28;01mif\u001b[39;00m result.prompt_eval_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m    651\u001b[39m     completion_tokens=(result.eval_count \u001b[38;5;28;01mif\u001b[39;00m result.eval_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m),\n\u001b[32m    652\u001b[39m )\n\u001b[32m    654\u001b[39m logger.info(\n\u001b[32m    655\u001b[39m     LLMCallEvent(\n\u001b[32m    656\u001b[39m         messages=[m.model_dump() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m create_params.messages],\n\u001b[32m   (...)\u001b[39m\u001b[32m    660\u001b[39m     )\n\u001b[32m    661\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\autogen\\autogen3\\Lib\\site-packages\\ollama\\_client.py:854\u001b[39m, in \u001b[36mAsyncClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    809\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    810\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    818\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    819\u001b[39m ) -> Union[ChatResponse, AsyncIterator[ChatResponse]]:\n\u001b[32m    820\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    821\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    822\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    851\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns an asynchronous `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    852\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m    855\u001b[39m     ChatResponse,\n\u001b[32m    856\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    857\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m/api/chat\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    858\u001b[39m     json=ChatRequest(\n\u001b[32m    859\u001b[39m       model=model,\n\u001b[32m    860\u001b[39m       messages=\u001b[38;5;28mlist\u001b[39m(_copy_messages(messages)),\n\u001b[32m    861\u001b[39m       tools=\u001b[38;5;28mlist\u001b[39m(_copy_tools(tools)),\n\u001b[32m    862\u001b[39m       stream=stream,\n\u001b[32m    863\u001b[39m       think=think,\n\u001b[32m    864\u001b[39m       \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    865\u001b[39m       options=options,\n\u001b[32m    866\u001b[39m       keep_alive=keep_alive,\n\u001b[32m    867\u001b[39m     ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    868\u001b[39m     stream=stream,\n\u001b[32m    869\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\autogen\\autogen3\\Lib\\site-packages\\ollama\\_client.py:692\u001b[39m, in \u001b[36mAsyncClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    688\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    690\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_raw(*args, **kwargs)).json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\autogen\\autogen3\\Lib\\site-packages\\ollama\\_client.py:638\u001b[39m, in \u001b[36mAsyncClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    636\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "# Assuming your Ollama server is running locally on port 11434.\n",
    "ollama_model_client = OllamaChatCompletionClient(model=\"llama3.2\")\n",
    "\n",
    "response = await ollama_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "print(response)\n",
    "await ollama_model_client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finish_reason='stop' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=32, completion_tokens=8) cached=False logprobs=None thought=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "agent = AssistantAgent(\n",
    "    name='assistant',\n",
    "    model_client=ollama_model_client,\n",
    "    system_message='You are a helpful assistant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labrador Retrievers! Here's some interesting information about one of the most popular breeds in the world:\n",
      "\n",
      "**Origin**\n",
      "-----------------\n",
      "\n",
      "The Labrador Retriever originated in Newfoundland, Canada, where they were bred to assist fishermen and retrieve fish. The breed was developed by crossing local dogs with other breeds, including the St. John's Water Dog.\n",
      "\n",
      "**Physical Characteristics**\n",
      "-----------------------------\n",
      "\n",
      "* **Size:** Males: 22.5-24.5 inches (57-62 cm) tall at the shoulder; females: 21.5-23.5 inches (55-60 cm)\n",
      "* **Weight:** 55-80 pounds (25-36 kg)\n",
      "* **Coat:** Short, dense, and smooth\n",
      "* **Color:** Black, yellow, or chocolate\n",
      "\n",
      "**Personality**\n",
      "-----------------\n",
      "\n",
      "Labrador Retrievers are known for their friendly, outgoing, and energetic personalities. They:\n",
      "\n",
      "* Are highly social and love people\n",
      "* Are excellent family dogs and are often used as therapy dogs\n",
      "* Are highly intelligent and trainable\n",
      "* Love to please their owners and enjoy pleasing them with good behavior\n",
      "\n",
      "**Health**\n",
      "-------------\n",
      "\n",
      "Labrador Retrievers are generally a healthy breed, but like all breeds, they can be prone to certain health issues:\n",
      "\n",
      "* Hip dysplasia\n",
      "* Elbow dysplasia\n",
      "* Obesity\n",
      "* Eye problems (e.g., cataracts, progressive retinal atrophy)\n",
      "* Allergies\n",
      "\n",
      "**Grooming**\n",
      "-------------\n",
      "\n",
      "Labrador Retrievers have a short, dense coat that requires minimal grooming. They need to be brushed occasionally to remove loose hair and distribute skin oils.\n",
      "\n",
      "**Exercise Needs**\n",
      "-------------------\n",
      "\n",
      "Labrador Retrievers are high-energy dogs that require regular exercise to stay happy and healthy:\n",
      "\n",
      "* Daily walks (at least 30 minutes)\n",
      "* Playtime (e.g., fetch, running, swimming)\n",
      "* Mental stimulation (e.g., obedience training, puzzle toys)\n",
      "\n",
      "**Fun Facts**\n",
      "----------------\n",
      "\n",
      "* Labradors are one of the most popular breeds in the world, according to the American Kennel Club.\n",
      "* They were originally bred as hunting dogs, but today they're often used as service dogs, therapy dogs, and family pets.\n",
      "* The breed's name comes from their country of origin (Labrador) and their original purpose (retrieving fish).\n",
      "\n",
      "I hope you found this information helpful! Do you have any specific questions about Labrador Retrievers?\n"
     ]
    }
   ],
   "source": [
    "result = await agent.run(task='Find information about Labrador Retriever')\n",
    "print(result.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
